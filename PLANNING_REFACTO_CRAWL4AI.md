# üìã Plan de Refactorisation du Crawler avec Crawl4AI

## 1. **Audit & Nettoyage du Code Actuel**
- [x] Lister tous les fichiers/fonctions li√©s au crawling/scraping.
- [x] Identifier les parties non conformes √† la doc officielle ([docs](https://docs.crawl4ai.com/)).
- [x] Supprimer ou isoler le code spaghetti, les duplications, les hacks temporaires.

> **Note :** Le dossier `crawler1/` a √©t√© identifi√© comme code legacy. Il sera archiv√© sous le nom `crawler_legacy/` pour r√©f√©rence temporaire, puis supprim√© apr√®s migration √©ventuelle d'utilitaires utiles.

---

## 2. **Structuration du Projet**
- [ ] Cr√©er un dossier `crawler/` ou `scraper/` d√©di√©.
- [ ] Cr√©er les fichiers suivants :
  - [ ] `main_crawler.py` (point d'entr√©e principal pour lancer le crawl)
  - [ ] `config.py` (pour les configurations sp√©cifiques au crawler)
  - [ ] `database_handler.py` (pour l'insertion dans PostgreSQL)
  - [ ] `models.py` (pour les sch√©mas de donn√©es du crawler si diff√©rents de l'API)
  - [ ] `custom_crawler.py` (si des logiques de crawling sp√©cifiques sont n√©cessaires)

---

## 3. **Impl√©mentation du Crawler selon Crawl4AI**

### 3.1. **Configuration du Crawler**
- [ ] D√©finir la configuration de Crawl4AI en utilisant la classe `CrawlerConfig`.
- [ ] Configurer les `user_agent`, `cache_enabled`, `proxy_enabled`, `headless`.
- [ ] Int√©grer les param√®tres de `max_pages`, `max_depth`, `max_requests_per_second`.

### 3.2. **D√©finition des strat√©gies de crawling**
- [ ] Utiliser `WebCrawler` pour les pages web standard.
- [ ] Utiliser `AsyncWebCrawler` si des t√¢ches asynchrones sont n√©cessaires.
- [ ] Envisager `LLMExtractionStrategy` pour l'extraction s√©mantique avanc√©e.
- [ ] Mettre en place des `selectors` (CSS, XPath) pour cibler les √©l√©ments.
- [ ] Utiliser `regex_patterns` pour l'extraction de donn√©es complexes.

### 3.3. **Gestion des pages**
- [ ] Impl√©menter la fonction `process_page` pour traiter chaque page visit√©e.
- [ ] Extraire les donn√©es pertinentes (titre, description, URL, entreprise, etc.).
- [ ] G√©rer la pagination et les liens internes/externes.

### 3.4. **Gestion des erreurs et logs**
- [ ] Utiliser les m√©canismes de gestion d'erreurs de Crawl4AI (`on_error`, `on_4xx`, `on_5xx`).
- [ ] Mettre en place un syst√®me de logging (`logging` de Python) pour suivre l'avancement.

---

## 4. **Int√©gration de la Base de Donn√©es (PostgreSQL)**
- [ ] D√©placer la logique de connexion et d'insertion PostgreSQL dans `database_handler.py`.
- [ ] Assurer la compatibilit√© des types de donn√©es Python avec PostgreSQL.
- [ ] Utiliser `asyncpg` pour les op√©rations asynchrones sur la base de donn√©es.
- [ ] G√©rer les transactions pour l'insertion des donn√©es.
- [ ] Impl√©menter la gestion des doublons et des mises √† jour (upsert).

---

## 5. **Tests & Validation**
- [ ] Cr√©er des tests unitaires pour les fonctions d'extraction et d'insertion.
- [ ] Cr√©er des tests d'int√©gration pour v√©rifier le cycle complet (crawl -> insertion).
- [ ] Valider les donn√©es ins√©r√©es directement dans la base de donn√©es.
- [ ] Ex√©cuter le crawler sur un petit ensemble de pages pour valider le fonctionnement.

---

## 6. **Optimisation & D√©ploiement**
- [ ] Optimiser les s√©lecteurs CSS/XPath pour am√©liorer les performances.
- [ ] Mettre en place le cache de Crawl4AI pour √©viter les requ√™tes inutiles.
- [ ] G√©rer la rotation des proxys si n√©cessaire.
- [ ] Envisager le d√©ploiement en Docker si le volume de crawl est important.

---

## 7. **Maintenance & √âvolution**
- [ ] Mettre √† jour r√©guli√®rement Crawl4AI pour b√©n√©ficier des derni√®res am√©liorations.
- [ ] Surveiller les logs et les performances du crawler en production.
- [ ] Adapter le crawler aux changements de structure des sites web cibl√©s.

---

## **Ressources Cl√©s**
- **Documentation Crawl4AI :** [https://docs.crawl4ai.com/](https://docs.crawl4ai.com/)
- **D√©p√¥t GitHub Crawl4AI :** [https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)


Ce plan sera ton guide. Nous allons avancer √©tape par √©tape pour le mettre en ≈ìuvre. Es-tu pr√™t √† commencer par l'audit et le nettoyage du code actuel ? 