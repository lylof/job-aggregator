# Documentation Compl√®te et Enrichie

## Plateforme d‚ÄôAgr√©gation d‚ÄôOffres d‚ÄôEmploi ‚Äì Version Mise √† Jour

---

## Table des Mati√®res

1. [Introduction & Contexte](#1-introduction--contexte)
2. [Objectifs du MVP](#2-objectifs-du-mvp)
3. [Architecture Globale](#3-architecture-globale)

   1. [Sch√©ma de principe](#31-sch√©ma-de-principe)
   2. [Flux de Donn√©es : Scraping ‚Üí API ‚Üí Bots](#32-flux-de-donn√©es--scraping----api----bots)
4. [Composants Techniques](#4-composants-techniques)

   1. [Module 1 : Scraping (Crawl4AI)](#41-module-1--scraping-crawl4ai)
   2. [Module 2 : Base de Donn√©es](#42-module-2--base-de-donn√©es)
   3. [Module 3 : API & Mini-Site Web](#43-module-3--api--mini-site-web)
   4. [Module 4 : Bots de Diffusion (Telegram, WhatsApp, Twitter‚Ä¶)](#44-module-4--bots-de-diffusion-telegram-whatsapp-twitter)
   5. [Module 5 : Bot Admin (Alertes & Monitoring)](#45-module-5--bot-admin-alertes--monitoring)
5. [Exemples de Configuration](#5-exemples-de-configuration)

   1. [Docker Compose : `docker-compose.yml`](#51-docker-compose--docker-composeyml)
   2. [Exemple de `Dockerfile` pour Crawl4AI](#52-exemple-de-dockerfile-pour-crawl4ai)
   3. [Exemple de Script (run\_crawl4ai.py)](#53-exemple-de-script-run_crawl4aipy)
   4. [Exemple de Dockerfile pour l‚ÄôAPI FastAPI](#54-exemple-de-dockerfile-pour-lapi-fastapi)
   5. [Exemple de Code pour le Bot Telegram ‚ÄúOffres‚Äù](#55-exemple-de-code-pour-le-bot-telegram-offres)
   6. [Exemple de Code pour le Bot WhatsApp (Twilio)](#56-exemple-de-code-pour-le-bot-whatsapp-twilio)
6. [Plan de D√©ploiement & Mise en ≈íuvre](#6-plan-de-d√©ploiement--mise-en-≈ìuvre)

   1. [Pr√©paration du VPS](#61-pr√©paration-du-vps)
   2. [Installation de Docker & Docker Compose](#62-installation-de-docker--docker-compose)
   3. [Structure des R√©pertoires](#63-structure-des-r√©pertoires)
   4. [Configuration du Cron](#64-configuration-du-cron)
   5. [D√©ploiement Frontend sur Vercel](#65-d√©ploiement-frontend-sur-vercel)
7. [Gestion du Stockage & Base de Donn√©es](#7-gestion-du-stockage--base-de-donn√©es)

   1. [Sch√©ma de la Base `offers`](#71-sch√©ma-de-la-base-offers)
   2. [Migration SQLite ‚Üí PostgreSQL (Phase 2)](#72-migration-sqlite--postgresql-phase-2)
8. [Surveillance, Logs & Alerte](#8-surveillance-logs--alerte)

   1. [Logs Docker & Heap](#81-logs-docker--heap)
   2. [Bot Admin : M√©canisme d‚ÄôAlerte](#82-bot-admin--m√©canisme-dalerte)
   3. [Healthchecks & Red√©marrage Automatique](#83-healthchecks--red√©marrage-automatique)
9. [Estimation des Ressources & Co√ªts](#9-estimation-des-ressources--co√ªts)

   1. [Usage M√©moire et CPU](#91-usage-m√©moire-et-cpu)
   2. [Bande Passante & Stockage](#92-bande-passante--stockage)
   3. [Budget Mensuel](#93-budget-mensuel)
10. [Roadmap & Phases d‚Äô√âvolution](#10-roadmap--phases-d√©volution)

    1. [Phase 1 : MVP Essentiel](#101-phase-1--mvp-essentiel)
    2. [Phase 2 : Am√©liorations IA & Performance](#102-phase-2--am√©liorations-ia--performance)
    3. [Phase 3 : Extensions & Modules Premium](#103-phase-3--extensions--modules-premium)
11. [Annexes & Ressources Utiles](#11-annexes--ressources-utiles)

    1. [Prompts IA Recommand√©s](#111-prompts-ia-recommand√©s)
    2. [Biblioth√®que & D√©p√¥ts GitHub Conseill√©s](#112-biblioth√®que--d√©p√¥ts-github-conseill√©s)
    3. [Checklist de Validation](#113-checklist-de-validation)

---

## 1. Introduction & Contexte

Tu d√©veloppes un **MVP de plateforme d‚Äôagr√©gation d‚Äôoffres d‚Äôemploi**.
L‚Äôenjeu est de r√©cup√©rer automatiquement les nouvelles offres publi√©es sur plusieurs sites (minimum 15), de les stocker en base, de les exposer via un mini-site web, puis de diffuser ces offres sur des canaux externes (Telegram, WhatsApp, Twitter‚Ä¶).

> **Contraintes cl√©s** :
>
> * **Ressources limit√©es** : VPS VLE-2 √† 2 vCores / 2 Go RAM / 40 Go NVMe (4,25 ‚Ç¨ HT/mois).
> * **Automatisation ‚Äúbout en bout‚Äù** : le scraper doit fonctionner 4√ó/jour, stocker en base, l‚ÄôAPI doit toujours √™tre √† jour, et les bots se contentent de consommer l‚ÄôAPI (sans acc√®s direct √† la base).
> * **Adaptabilit√©** : les sites d‚Äôoffres √©voluent r√©guli√®rement, il faut minimiser la maintenance manuelle.
> * **Co√ªt minimal** : pr√©f√©rer des solutions open source et gratuites, utiliser des mod√®les LLM gratuits (Gemini, Qwen, DeepSeek via OpenRouter).

Nous adoptons donc une architecture **modulaire** en 5 composants principaux :

1. **Scraping (Crawl4AI)**
2. **Base de donn√©es (SQLite ‚Üí PostgreSQL)**
3. **API / Mini-site Web (FastAPI + Next.js)**
4. **Bots de diffusion (Telegram, WhatsApp, Twitter‚Ä¶)**
5. **Bot Admin (pour alertes & monitoring)**

---

## 2. Objectifs du MVP

1. **Impact maximal √† moindre co√ªt**

   * Obtenir rapidement une liste d‚Äôoffres √† jour pour les candidats.
   * H√©berger l‚Äôensemble pour < 7 ‚Ç¨ / mois.

2. **Automatisation compl√®te**

   * Scraping 4√ó/jour, mise √† jour en base.
   * Bots consomment l‚ÄôAPI ‚Äú/offers?nouveaux=true‚Äù pour diffuser.
   * Aucune intervention humaine pour la diffusion (bots ind√©pendants).

3. **Maintenance minimale**

   * Scraper IA-first (Crawl4AI) s‚Äôadaptant aux petits changements de structure.
   * API unique servant le front-end et les bots.
   * Centralisation des filtres et r√®gles de ‚Äúnouveaux‚Äù offres dans l‚ÄôAPI.

4. **Architecture √©volutive**

   * Possibilit√© d‚Äôajouter de nouveaux canaux (WhatsApp, Twitter, LinkedIn‚Ä¶) en d√©veloppant un simple bot consommant l‚ÄôAPI.
   * Transition future vers micro-services (ScraperService, ApiService, BotService, etc.).
   * Pr√©voir l‚Äôajout √©ventuel d‚Äôun espace recruteur / premium (module payant).

---

## 3. Architecture Globale

### 3.1. Sch√©ma de principe

```mermaid
flowchart LR
  subgraph C1 [Scraping & Base]
    CronScheduler["cron (4√ó/jour)"]
    subgraph C1A [Crawl4AI Scrapers]
      C4AI1["Crawl4AI (Sites 1‚Äì5)"]
      C4AI2["Crawl4AI (Sites 6‚Äì10)"]
      C4AI3["Crawl4AI (Sites 11‚Äì15)"]
    end
    CronScheduler --> C4AI1
    CronScheduler --> C4AI2
    CronScheduler --> C4AI3
    C4AI1 & C4AI2 & C4AI3 --> DB[(SQLite /app/app.db)]
    C4AI1 & C4AI2 & C4AI3 -->|Alertes Erreur| BotAdmin[(Bot Admin)]
    C4AI1 & C4AI2 & C4AI3 -->|Prompts LLM| LLM[(Gemini/Qwen via OpenRouter)]
  end

  subgraph C2 [API & Frontend]
    DB --> API["API FastAPI (app:5000)"]
    API --> Frontend["Mini-Site Next.js (Vercel)"]
  end

  subgraph C3 [Bots de Diffusion]
    BotTG["Bot Telegram"]
    BotWA["Bot WhatsApp"]
    BotX["Bot Twitter"]
    API -->|GET /offers?nouveaux=true| BotTG
    API -->|GET /offers?nouveaux=true| BotWA
    API -->|GET /offers?nouveaux=true| BotX
  end

  classDef comp fill:#f3f3f3,stroke:#333,stroke-width:1px;
  class C1,C2,C3 comp;
```

1. **Cron Scheduler** : d√©clenche 4√ó/jour les conteneurs `Crawl4AI1`, `Crawl4AI2`, `Crawl4AI3`, chacun g√©rant 5 des 15 sites.

2. **Crawl4AI Scrapers** :

   * Ex√©cutent un ‚Äúcrawl‚Äù sur les pages de type ‚Äújob board‚Äù.
   * Ne r√©cup√®rent que les **nouvelles offres** (contr√¥le via `url` unique ou champ `date_scraped`).
   * Envoient les donn√©es structur√©es (JSON) vers la base SQLite (`app.db`).
   * En cas d‚Äôerreur de scraping (s√©lecteur introuvable, page inaccessible), d√©clenchent un appel au **Bot Admin** pour alerter, et/ou font un prompt au **LLM** pour tenter une correction automatique.

3. **Base de donn√©es (SQLite /app/app.db)** :

   * Contient la table `offers` avec tous les champs n√©cessaires
   * Conserve l‚Äôhistorique, permet le filtrage ‚Äúnouveaux‚Äù

4. **API (FastAPI)** :

   * Expose les endpoints :

     * `GET /offers` (pagination, filtres),
     * `GET /offers?nouveaux=true` (retourne les offres ins√©r√©es depuis la derni√®re ex√©cution),
     * `GET /offer/{id}` (d√©tails),
     * `GET /filters` (liste distincte des `location`, `contract_type`).
   * Sert √† la fois le **frontend Next.js** et les **bots de diffusion**.
   * Impl√©mente la logique ‚Äúmarque comme lu‚Äù ou ‚Äúwindow temporelle‚Äù pour √©viter les doublons dans les bots.

5. **Mini-Site (Next.js sur Vercel)** :

   * Affiche la liste pagin√©e des offres, avec filtres (location, type de contrat, recherche full-text).
   * Page d√©tail `/offer/[id]` pour chaque offre.
   * Esth√©tique minimaliste (pas de header/hero/footer volumineux), design responsive.
   * Appelle l‚ÄôAPI via `fetch` (ou `getServerSideProps`) pour construire les pages.

6. **Bots de diffusion** :

   * **Bot Telegram :**

     * Se connecte toutes les heures √† `GET /offers?nouveaux=true`, prend les nouvelles offres, formate un message Markdown, envoie dans un canal Telegram.
   * **Bot WhatsApp** (via Twilio ou Baileys) :

     * M√™me principe, envoie un message WhatsApp pour chaque nouvelle offre.
   * **Bot Twitter** (via l‚ÄôAPI X) :

     * Publie un tweet pour chaque nouvelle offre (respect de 280 caract√®res ou lien raccourci).
   * Tous ces bots ne touchent pas directement √† la base, mais ne font qu‚Äôappeler l‚Äô**API** du mini-site.

7. **Bot Admin (Telegram)** :

   * Re√ßoit les alertes de scraping (via webhook ou appel direct).
   * Pour chaque conteneur `Crawl4AI` en erreur, re√ßoivent un message (`‚ÄúErreur sur Site X : impossible d‚Äôextraire titre. Correction en cours‚Ä¶‚Äù`).
   * Peut √©galement envoyer un message si un conteneur ne remonte pas (via un scheduler de v√©rification).

---

## 4. Composants Techniques

### 4.1. Module 1 : Scraping (Crawl4AI)

#### 4.1.1. Pr√©sentation de Crawl4AI

* **Projet** : Open source, \~ 44 400 ‚òÖ sur GitHub (2023+).
* **Langage** : Python (3.10+).
* **Fonctionnalit√©s cl√©s** :

  * Mode ‚Äústealth‚Äù (contournement d‚Äôanti-bot de base).
  * Int√©gration simplifi√©e de Playwright.
  * Possibilit√© d‚Äôutiliser un **LLM** (via OpenRouter, HuggingFace, local) pour analyser le DOM et d√©terminer automatiquement les s√©lecteurs.
  * G√©n√©ration de JSON structur√© directement, pr√™t √† ins√©rer en base.
  * Support du crawling en pipeline (d√©couverte de liens, exploration de pages, extraction).
  * Syst√®me de ‚Äúconfig.json‚Äù pour d√©crire chaque site (URL, prompts LLM, r√®gles de pagination, s√©lecteurs par d√©faut).

#### 4.1.2. Pourquoi Crawl4AI pour ce projet ?

* **Adaptabilit√©** :

  * Si un site modifie ses classes CSS, le LLM peut recevoir un prompt ¬´ Ne trouve pas .job-title, trouve le nouvel √©l√©ment affichant le titre ¬ª et ajuster automatiquement le s√©lecteur.
  * Cela r√©duit la maintenance manuelle, car seuls les changements ‚Äúradicaux‚Äù (nouvelle architecture HTML compl√®te) n√©cessitent une intervention humaine.
* **Performance** :

  * Consomme \~ 500 Mo RAM par conteneur (Playwright + Python).
  * CPU 10 %‚Äì15 % au moment du crawl (pour 5 sites).
  * Facteur d√©cisif pour un VPS √† 2 Go.

#### 4.1.3. Structure du r√©pertoire `crawlerX/`

```
crawler1/
‚îú‚îÄ Dockerfile
‚îú‚îÄ requirements.txt
‚îú‚îÄ run_crawl4ai.py
‚îî‚îÄ config.json
```

* `requirements.txt` :

  ```
  crawl4ai==3.0.0
  playwright
  python-dotenv
  ```
* `config.json` : liste des 5 sites, ex :

  ```json
  {
    "sites": [
      {
        "name": "EmploiTG",
        "url": "https://emploi.tg/offres",
        "prompt": "Pour chaque offre sur cette page, extraits le titre, l'entreprise, la localisation, la date, le type de contrat et l'URL. Ne r√©cup√®re que les offres publi√©es dans les derni√®res 6 heures."
      },
      {
        "name": "Jobrapido",
        "url": "https://www.jobrapido.com/emplois",
        "prompt": "Sur la page principale, parcours la liste des offres d'emploi, r√©cup√®re les √©l√©ments (title, company, location, url, posted_date) pour celles datant des derni√®res 6 heures."
      },
      // 3 autres blocs similaires‚Ä¶
    ],
    "llm_provider": "openrouter",
    "llm_model": "qwen-1",
    "llm_api_key": "<OPENROUTER_API_KEY>"
  }
  ```
* `run_crawl4ai.py` : le script principal pour lancer le crawl sur les 5 sites. Voir section [5.3](#53-exemple-de-script-run_crawl4aipy).

### 4.2. Module 2 : Base de Donn√©es

#### 4.2.1. Choix de SQLite pour le MVP

* **Avantages** :

  * Fichier unique, zero configuration.
  * Suffisant pour un volume de quelques dizaines de milliers d‚Äôoffres.
  * Acc√®s rapide (local).
* **Limites** :

  * Moindre scalabilit√© si < de 100 000 enregistrements, mais on peut migrer vers PostgreSQL au besoin (Phase 2).
  * Verrouillage possible si trop d‚Äô√©critures simultan√©es, mais notre planner ex√©cute s√©quentiellement les crawlers.

#### 4.2.2. Sch√©ma de la table `offers`

```sql
CREATE TABLE IF NOT EXISTS offers (
  id              INTEGER PRIMARY KEY AUTOINCREMENT,
  url             TEXT UNIQUE NOT NULL,
  title           TEXT NOT NULL,
  company         TEXT,
  location        TEXT,
  date_posted     TEXT,   -- ex: '2025-05-28T14:00:00Z'
  contract_type   TEXT,
  date_scraped    TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  is_new          BOOLEAN DEFAULT 1
);
```

* `url` : cl√© unique (emp√™che les doublons).
* `date_posted` : date/heure de publication sur le site d‚Äôorigine (ou approxim√©e).
* `date_scraped` : timestamp UTC de l‚Äôinsertion.
* `is_new` : bool√©en temporaire pour indiquer qu‚Äôune offre n‚Äôa pas encore √©t√© lue par l‚ÄôAPI / Bot.

> **Logique ‚Äúis\_new‚Äù** :
>
> * Lorsqu‚Äôun scraper ins√®re une offre, `is_new = 1`.
> * L‚ÄôAPI, lors d‚Äôun `GET /offers?nouveaux=true`, r√©cup√®re les enregistrements `WHERE is_new = 1` puis **met √† jour** ces lignes en `is_new = 0`.
> * Ainsi, un bot n‚Äôenverra chaque offre qu‚Äôune seule fois (pas de doublons).

#### 4.2.3. Migration vers PostgreSQL (Phase 2)

* **Pourquoi** :

  * Besoin de plus de fiabilit√© sous haute charge, meilleures performances sur les requ√™tes concurrentes, indexations avanc√©es.
* **Comment** :

  1. Installer PostgreSQL (service g√©r√© comme **ElephantSQL** ou **Heroku Postgres Free**).
  2. Exporter `offers` depuis SQLite :

     ```bash
     sqlite3 app.db .dump > sqlite_dump.sql
     ```
  3. Modifier le script d‚Äôimportation (`psql`) pour recr√©er la table en PostgreSQL et importer les donn√©es.
  4. Adapter `DATABASE_URL` dans `FastAPI` pour pointer vers `postgresql://user:pass@host:port/dbname`.
  5. Tester les requ√™tes via `psycopg2` ou `asyncpg` (FastAPI en mode async).

---

### 4.3. Module 3 : API & Mini-Site Web

#### 4.3.1. API (FastAPI)

##### 4.3.1.1. Pr√©sentation

* **Rapide**, **l√©ger** et **asynchrone**.
* G√©n√®re automatiquement la documentation OpenAPI/Swagger √† `/docs`.
* Connecte directement avec SQLite ou PostgreSQL (via SQLAlchemy ou `databases`).

##### 4.3.1.2. Endpoints principaux

| Endpoint                    | Description                                                                                            |
| --------------------------- | ------------------------------------------------------------------------------------------------------ |
| `GET /offers`               | Retourne la liste pagin√©e des offres (params : `page`, `page_size`, `location`, `contract_type`).      |
| `GET /offers?nouveaux=true` | Retourne uniquement les offres avec `is_new = 1`, puis met `is_new` √† 0.                               |
| `GET /offer/{id}`           | Retourne tous les champs d‚Äôune offre (titre, entreprise, location, date\_posted, contract\_type, url). |
| `GET /filters`              | Retourne JSON :                                                                                        |

```json
{
  "locations": ["Lom√©", "Accra", "Abidjan", ...],
  "contract_types": ["CDD", "CDI", "Stage", ‚Ä¶]
}
```

##### 4.3.1.3. Exemple succinct (Python | FastAPI)

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import sqlite3
from typing import List, Optional
import datetime

app = FastAPI(title="Job Aggregator API")

DATABASE = "app.db"

class Offer(BaseModel):
    id: int
    title: str
    company: Optional[str]
    location: Optional[str]
    date_posted: Optional[str]
    contract_type: Optional[str]
    url: str
    date_scraped: str

def get_db():
    conn = sqlite3.connect(DATABASE)
    conn.row_factory = sqlite3.Row
    return conn

@app.get("/offers", response_model=List[Offer])
def list_offers(
    page: int = 1,
    page_size: int = 10,
    location: Optional[str] = None,
    contract_type: Optional[str] = None,
    nouveaux: bool = False
):
    conn = get_db()
    cursor = conn.cursor()

    if nouveaux:
        # R√©cup√®re et marque comme lues
        cursor.execute("SELECT * FROM offers WHERE is_new = 1 ORDER BY date_scraped DESC")
        rows = cursor.fetchall()
        cursor.execute("UPDATE offers SET is_new = 0 WHERE is_new = 1")
        conn.commit()
    else:
        query = "SELECT * FROM offers"
        params = []
        filters = []
        if location:
            filters.append("location = ?")
            params.append(location)
        if contract_type:
            filters.append("contract_type = ?")
            params.append(contract_type)
        if filters:
            query += " WHERE " + " AND ".join(filters)
        query += " ORDER BY date_scraped DESC LIMIT ? OFFSET ?"
        params.extend([page_size, (page-1)*page_size])
        cursor.execute(query, params)
        rows = cursor.fetchall()

    conn.close()
    return [Offer(**dict(row)) for row in rows]

@app.get("/offer/{offer_id}", response_model=Offer)
def get_offer(offer_id: int):
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM offers WHERE id = ?", (offer_id,))
    row = cursor.fetchone()
    conn.close()
    if not row:
        raise HTTPException(status_code=404, detail="Offer not found")
    return Offer(**dict(row))

@app.get("/filters")
def get_filters():
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute("SELECT DISTINCT location FROM offers")
    locations = [r[0] for r in cursor.fetchall() if r[0]]
    cursor.execute("SELECT DISTINCT contract_type FROM offers")
    contract_types = [r[0] for r in cursor.fetchall() if r[0]]
    conn.close()
    return {"locations": sorted(locations), "contract_types": sorted(contract_types)}
```

###### Points cl√©s :

* `nouveaux=true` g√®re automatiquement la mise √† jour de `is_new`.
* On ferme la connexion √† chaque requ√™te pour lib√©rer le fichier SQLite.
* Pour PostgreSQL, remplacer `sqlite3` par `asyncpg` ou `psycopg2` + SQLAlchemy.

---

#### 4.3.2. Mini-Site Web (Next.js)

##### 4.3.2.1. Pr√©sentation

* **Stack** : Next.js (React + Server Side Rendering ou Static Generation) avec **Tailwind CSS** pour le style.
* **H√©bergement** : Vercel (plan gratuit).
* **Fonctionnalit√©s** :

  1. **Page d‚Äôaccueil `/`** :

     * Appel √† l‚ÄôAPI (`GET /offers?page=1&page_size=10`) pour charger la premi√®re page d‚Äôoffres.
     * Affiche les offres sous forme de **carte** (JobCard).
     * Sidebar (Desktop) ou Drawer (Mobile) pour filtres (location, contract\_type).
     * Barre de recherche entr√©e libre (full-text sur `title`).
  2. **Pagination** : ‚ÄúPr√©c√©dent‚Äù / ‚ÄúSuivant‚Äù, 10 offres par page.
  3. **Page d√©tail `/offer/[id]`** :

     * Appel √† `GET /offer/{id}` et affiche titre, entreprise, localisation, date\_posted, contract\_type, lien ‚ÄúPostuler‚Äù vers l‚ÄôURL externe.
  4. **Filtres dynamiques** :

     * Exemples de composants :

       * **Liste d√©roulante (Dropdown)** pour `location`.
       * **Checkbox Group** pour `contract_type`.
       * **Champ de recherche** qui filtre en temps r√©el en appelant `GET /offers` avec query param `search=texte`.
  5. **Design** :

     * Cartes blanches (`bg-white`) avec ombres l√©g√®res (`shadow-sm`), coins arrondis (`rounded-lg`).
     * Boutons styl√©s (`bg-blue-600 hover:bg-blue-700 text-white rounded-md`) pour la pagination et ‚ÄúPostuler‚Äù.
     * Responsive mobile-first (grille simple, Drawer pour filtres).

##### 4.3.2.2. Structure des fichiers (exemple)

```
frontend/
‚îú‚îÄ components/
‚îÇ  ‚îú‚îÄ JobCard.jsx
‚îÇ  ‚îú‚îÄ FilterSidebar.jsx
‚îÇ  ‚îî‚îÄ Pagination.jsx
‚îú‚îÄ pages/
‚îÇ  ‚îú‚îÄ index.jsx
‚îÇ  ‚îî‚îÄ offer/
‚îÇ     ‚îî‚îÄ [id].jsx
‚îú‚îÄ styles/
‚îÇ  ‚îî‚îÄ globals.css
‚îú‚îÄ tailwind.config.js
‚îú‚îÄ next.config.js
‚îú‚îÄ package.json
‚îî‚îÄ README.md
```

###### Exemple de composant `JobCard.jsx`

```jsx
import React from 'react';

export default function JobCard({ offer }) {
  return (
    <div className="bg-white shadow-sm rounded-lg p-4 flex flex-col md:flex-row justify-between items-start">
      <div>
        <h2 className="text-lg font-semibold">{offer.title}</h2>
        <p className="text-sm text-gray-600">{offer.company} ‚Äì {offer.location}</p>
        <p className="text-xs text-gray-500 mt-1">Publi√© : {new Date(offer.date_posted).toLocaleString()}</p>
      </div>
      <a
        href={offer.url}
        target="_blank"
        rel="noopener noreferrer"
        className="mt-4 md:mt-0 bg-blue-600 hover:bg-blue-700 text-white px-3 py-1 rounded-md"
      >
        Voir d√©tails
      </a>
    </div>
  );
}
```

###### Exemple de page principale `index.jsx`

```jsx
import React, { useState, useEffect } from 'react';
import JobCard from '../components/JobCard';
import FilterSidebar from '../components/FilterSidebar';
import Pagination from '../components/Pagination';

export default function Home() {
  const [offers, setOffers] = useState([]);
  const [filters, setFilters] = useState({ locations: [], contract_types: [] });
  const [selectedLocation, setSelectedLocation] = useState('');
  const [selectedContract, setSelectedContract] = useState('');
  const [searchTerm, setSearchTerm] = useState('');
  const [page, setPage] = useState(1);
  const pageSize = 10;

  useEffect(() => {
    fetchFilters();
  }, []);

  useEffect(() => {
    fetchOffers();
  }, [page, selectedLocation, selectedContract, searchTerm]);

  const fetchFilters = async () => {
    const res = await fetch('https://api.monsite.com/filters');
    const data = await res.json();
    setFilters(data);
  };

  const fetchOffers = async () => {
    const params = new URLSearchParams({
      page,
      page_size: pageSize,
      ...(selectedLocation && { location: selectedLocation }),
      ...(selectedContract && { contract_type: selectedContract }),
      ...(searchTerm && { search: searchTerm }),
    });
    const res = await fetch(`https://api.monsite.com/offers?${params}`);
    const data = await res.json();
    setOffers(data);
  };

  return (
    <div className="min-h-screen bg-gray-50 flex">
      <div className="hidden md:block w-1/4 p-4">
        <FilterSidebar
          filters={filters}
          selectedLocation={selectedLocation}
          setSelectedLocation={setSelectedLocation}
          selectedContract={selectedContract}
          setSelectedContract={setSelectedContract}
        />
      </div>
      <div className="flex-1 p-4">
        <div className="mb-4 flex items-center">
          <input
            type="text"
            placeholder="Rechercher des mots-cl√©s..."
            className="flex-1 border border-gray-300 rounded-md px-3 py-2"
            value={searchTerm}
            onChange={(e) => setSearchTerm(e.target.value)}
          />
        </div>
        <div className="space-y-4">
          {offers.map((offer) => (
            <JobCard key={offer.id} offer={offer} />
          ))}
        </div>
        <Pagination currentPage={page} setPage={setPage} />
      </div>
    </div>
  );
}
```

---

### 4.4. Module 4 : Bots de Diffusion

#### 4.4.1. Bot Telegram ‚ÄúOffres‚Äù

##### 4.4.1.1. Pr√©sentation

* **Librairie** : `python-telegram-bot` (version 20+).
* **R√¥le** : appeler p√©riodiquement `GET /offers?nouveaux=true`, formater un message Markdown pour chaque offre, et l‚Äôenvoyer dans un canal ou groupe Telegram pr√©-configur√©.

##### 4.4.1.2. Exemple de code (`bot_offres.py`)

```python
import os
import requests
import logging
from telegram import Bot
from telegram.error import TelegramError
from apscheduler.schedulers.blocking import BlockingScheduler

# Configuration
TOKEN = os.getenv("TELEGRAM_TOKEN")
API_URL = os.getenv("API_URL", "http://api_service:8000")
CHAT_ID = os.getenv("TG_CHANNEL_ID")  # ID du canal/groupe Telegram

# Initialisation
bot = Bot(token=TOKEN)
scheduler = BlockingScheduler()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fetch_and_send_new_offers():
    try:
        res = requests.get(f"{API_URL}/offers?nouveaux=true")
        res.raise_for_status()
        data = res.json()
        offers = data if isinstance(data, list) else data.get("offers", [])
        for off in offers:
            titre = off.get("title", "N/A")
            entreprise = off.get("company", "N/A")
            location = off.get("location", "N/A")
            url = off.get("url", "#")
            date_posted = off.get("date_posted", "")
            text = (
                f"üìå *{titre}*  \n"
                f"üè¢ {entreprise}  \n"
                f"üìç {location}  \n"
                f"‚è∞ Publi√©e : {date_posted}  \n"
                f"üîó [Voir l'offre]({url})"
            )
            bot.send_message(chat_id=CHAT_ID, text=text, parse_mode="Markdown")
            logger.info(f"Offre envoy√©e : {titre} ({entreprise})")
    except TelegramError as te:
        logger.error(f"Erreur envoi telgram : {te}")
    except Exception as e:
        logger.error(f"Erreur fetch/send offers : {e}")

# Envoi au d√©marrage
fetch_and_send_new_offers()

# Planification toutes les heures
scheduler.add_job(fetch_and_send_new_offers, 'interval', hours=1)
scheduler.start()
```

###### Points cl√©s :

* `apscheduler` g√®re une planification en boucle infinie.
* Le script s‚Äôex√©cute en tant que container Docker d√©marr√©, sans intervention manuelle.
* `CHAT_ID` peut √™tre l‚Äôidentifiant d‚Äôun canal Telegram (p. ex. `@mon_canal`).

---

#### 4.4.2. Bot WhatsApp (via Twilio)

##### 4.4.2.1. Pr√©sentation

* **Librairie** : `twilio` Python (biblioth√®que officielle).
* **R√¥le** : appeler `GET /offers?nouveaux=true`, formater un message texte et l‚Äôenvoyer via l‚ÄôAPI Twilio WhatsApp.

##### 4.4.2.2. Exemple de code (`bot_whatsapp.py`)

```python
import os
import requests
from twilio.rest import Client
import logging
from apscheduler.schedulers.blocking import BlockingScheduler

# Configuration Twilio
ACCOUNT_SID = os.getenv("TWILIO_ACCOUNT_SID")
AUTH_TOKEN = os.getenv("TWILIO_AUTH_TOKEN")
WHATSAPP_FROM = "whatsapp:+14155238886"
WHATSAPP_TO   = "whatsapp:+<TON_NUMERO>"

API_URL = os.getenv("API_URL", "http://api_service:8000")

client = Client(ACCOUNT_SID, AUTH_TOKEN)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fetch_and_send_whatsapp_offers():
    try:
        res = requests.get(f"{API_URL}/offers?nouveaux=true")
        res.raise_for_status()
        offers = res.json().get("offers", [])
        for off in offers:
            text = (
                f"{off.get('title', 'N/A')} ‚Äì {off.get('company', 'N/A')} ({off.get('location', 'N/A')})\n"
                f"Publi√© : {off.get('date_posted', '')}\n"
                f"Lien : {off.get('url', '')}"
            )
            message = client.messages.create(
                body=text,
                from_=WHATSAPP_FROM,
                to=WHATSAPP_TO
            )
            logger.info(f"WhatsApp envoy√© : {message.sid}")
    except Exception as e:
        logger.error(f"Erreur bot WhatsApp : {e}")

# Envoi initial
fetch_and_send_whatsapp_offers()

# Planification toutes les heures
scheduler = BlockingScheduler()
scheduler.add_job(fetch_and_send_whatsapp_offers, 'interval', hours=1)
scheduler.start()
```

###### Remarques :

* Tu dois param√©trer un compte **Twilio** (SID + Token) et lier ton num√©ro WhatsApp.
* La limite gratuite Twilio permet quelques dizaines de messages / jour ; surveille la console Twilio pour ajuster si besoin.

---

#### 4.4.3. Bot Twitter / X (optionnel)

##### 4.4.3.1. Pr√©sentation

* **Librairie** : `tweepy` (Python) ou `twitter-api-v2` (Node.js).
* **R√¥le** : publier automatiquement un tweet pour chaque nouvelle offre.
* **Limite** : maximal 300 Tweets / 3 h (compte standard).

##### 4.4.3.2. Exemple de code (Tweepy)

```python
import os
import requests
import logging
import tweepy
from apscheduler.schedulers.blocking import BlockingScheduler

# Config API X
API_KEY = os.getenv("TWITTER_API_KEY")
API_SECRET = os.getenv("TWITTER_API_SECRET")
ACCESS_TOKEN = os.getenv("TWITTER_ACCESS_TOKEN")
ACCESS_SECRET = os.getenv("TWITTER_ACCESS_TOKEN_SECRET")

auth = tweepy.OAuth1UserHandler(API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_SECRET)
twitter_api = tweepy.API(auth)

API_URL = os.getenv("API_URL", "http://api_service:8000")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fetch_and_tweet_offers():
    try:
        res = requests.get(f"{API_URL}/offers?nouveaux=true")
        res.raise_for_status()
        offers = res.json().get("offers", [])
        for off in offers:
            title = off.get("title", "")
            company = off.get("company", "")
            url = off.get("url", "")
            tweet = f"üîç {title} ‚Äì {company}\nLien : {url}"
            twitter_api.update_status(tweet)
            logger.info(f"Tweet√© : {title}")
    except Exception as e:
        logger.error(f"Erreur bot Twitter : {e}")

# Envoi initial
fetch_and_tweet_offers()

# Planification toutes les heures
scheduler = BlockingScheduler()
scheduler.add_job(fetch_and_tweet_offers, 'interval', hours=1)
scheduler.start()
```

> **Attention** :
>
> * Respecter la limite de 280 caract√®res.
> * G√©rer l‚Äô√©chappement d‚ÄôURL (utiliser `t.co` automatiquement).
> * Surveiller la politique d‚ÄôAPI X (limites JSON, etc.).

---

### 4.5. Module 5 : Bot Admin (Alertes & Monitoring)

#### 4.5.1. R√¥le et cas d‚Äôusage

* **Recevoir les alertes de crawl** : si un conteneur Crawl4AI √©choue (erreur d‚Äôextraction, page inaccessible), il envoie un message au Bot Admin (via un endpoint d√©di√© ou en appel direct).
* **Informer automatiquement l‚Äôadministrateur** (toi) qu‚Äôune action de maintenance est n√©cessaire.
* **Optionnel** : surveiller la fr√©quence de restart des conteneurs, surveiller le ‚Äúhealth‚Äù global.

#### 4.5.2. Exemple de code (Bot Admin ‚Äì `bot_admin.py`)

```python
import os
import logging
from telegram import Bot
from telegram.error import TelegramError
from flask import Flask, request, jsonify

app = Flask(__name__)

ADMIN_TOKEN = os.getenv("ADMIN_TELEGRAM_TOKEN")
ADMIN_CHAT_ID = os.getenv("ADMIN_CHAT_ID")
bot = Bot(token=ADMIN_TOKEN)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.route("/alert", methods=["POST"])
def alert():
    data = request.json
    site = data.get("site", "Inconnu")
    error = data.get("error", "Erreur non sp√©cifi√©e")
    message = f"‚ö†Ô∏è *Alerte Scraping* \nSite : {site}\nErreur : {error}"
    try:
        bot.send_message(chat_id=ADMIN_CHAT_ID, text=message, parse_mode="Markdown")
        return jsonify({"status": "ok"}), 200
    except TelegramError as te:
        logger.error(f"Erreur envoi Admin Bot : {te}")
        return jsonify({"status": "failed", "error": str(te)}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001)
```

###### Remarques :

* Chaque conteneur Crawl4AI, lorsqu‚Äôil rencontre une erreur, fait :

  ```python
  import requests
  requests.post("http://bot_admin:5001/alert", json={"site": "NomDuSite", "error": "CSS introuvable"})
  ```
* **Bot Admin** est un micro-service Flask (port 5001). Il pourrait √™tre int√©gr√© √† l‚ÄôAPI FastAPI, mais on le s√©pare pour all√©ger l‚ÄôAPI principale.
* `ADMIN_CHAT_ID` est l‚Äôidentifiant du canal ou du chat Telegram o√π tu souhaites √™tre alert√©.

---

## 5. Exemples de Configuration

### 5.1. Docker Compose : `docker-compose.yml` (racine)

```yaml
version: '3.8'
services:
  crawler1:
    build: ./crawler1
    container_name: crawler1
    restart: on-failure
    environment:
      - LLM_API_KEY=${LLM_API_KEY}       # Cl√© OpenRouter / Qwen / Gemini
      - DB_PATH=/app/app.db
      - BOT_ADMIN_URL=http://bot_admin:5001/alert
    volumes:
      - ./api/app.db:/app/app.db

  crawler2:
    build: ./crawler2
    container_name: crawler2
    restart: on-failure
    environment:
      - LLM_API_KEY=${LLM_API_KEY}
      - DB_PATH=/app/app.db
      - BOT_ADMIN_URL=http://bot_admin:5001/alert
    volumes:
      - ./api/app.db:/app/app.db

  crawler3:
    build: ./crawler3
    container_name: crawler3
    restart: on-failure
    environment:
      - LLM_API_KEY=${LLM_API_KEY}
      - DB_PATH=/app/app.db
      - BOT_ADMIN_URL=http://bot_admin:5001/alert
    volumes:
      - ./api/app.db:/app/app.db

  api:
    build: ./api
    container_name: api_service
    restart: always
    ports:
      - "8000:8000"
    volumes:
      - ./api/app.db:/app/app.db

  bot_offres:
    build: ./bot_offres
    container_name: bot_offres
    restart: always
    depends_on:
      - api
    environment:
      - TELEGRAM_TOKEN=${TELEGRAM_TOKEN}
      - API_URL=http://api_service:8000
      - TG_CHANNEL_ID=${TG_CHANNEL_ID}

  bot_whatsapp:
    build: ./bot_whatsapp
    container_name: bot_whatsapp
    restart: always
    depends_on:
      - api
    environment:
      - TWILIO_ACCOUNT_SID=${TWILIO_ACCOUNT_SID}
      - TWILIO_AUTH_TOKEN=${TWILIO_AUTH_TOKEN}
      - API_URL=http://api_service:8000
      - WHATSAPP_TO=${WHATSAPP_TO}

  bot_twitter:
    build: ./bot_twitter
    container_name: bot_twitter
    restart: always
    depends_on:
      - api
    environment:
      - TWITTER_API_KEY=${TWITTER_API_KEY}
      - TWITTER_API_SECRET=${TWITTER_API_SECRET}
      - TWITTER_ACCESS_TOKEN=${TWITTER_ACCESS_TOKEN}
      - TWITTER_ACCESS_SECRET=${TWITTER_ACCESS_SECRET}
      - API_URL=http://api_service:8000

  bot_admin:
    build: ./bot_admin
    container_name: bot_admin
    restart: always
    ports:
      - "5001:5001"
    environment:
      - ADMIN_TELEGRAM_TOKEN=${ADMIN_TELEGRAM_TOKEN}
      - ADMIN_CHAT_ID=${ADMIN_CHAT_ID}

networks:
  default:
    driver: bridge
```

> **Explications** :
>
> * **Volumes** : partagent le m√™me fichier `app.db` entre scrapers et API.
> * **Bots** : chacun a ses variables d‚Äôenvironnement propres (tokens).
> * **Bot Admin** : expose son port `5001` afin que les scrapers puissent y poster un JSON.
> * **LLM\_API\_KEY** est la m√™me pour les 3 conteneurs Crawl4AI (tu peux varier si tu utilises diff√©rents mod√®les).

---

### 5.2. Exemple de `Dockerfile` pour Crawl4AI

```dockerfile
# Fichier : crawler1/Dockerfile
FROM python:3.10-slim

# Variables d'environnement pour r√©duire la taille
ENV PIP_NO_CACHE_DIR=1 \
    PYTHONUNBUFFERED=1

# Installation des d√©pendances syst√®me pour Playwright
RUN apt-get update && \
    apt-get install -y wget gnupg ca-certificates libnss3 libatk1.0-0 libatk-bridge2.0-0 \
    libc6 libcairo2 libcups2 libdbus-1-3 libexpat1 libfontconfig1 libgbm1 libgcc1 libglib2.0-0 \
    libgtk-3-0 libnspr4 libpango-1.0-0 libx11-6 libx11-xcb1 libxcb1 libxcomposite1 libxcursor1 \
    libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxrender1 libxtst6 lsb-release \
    libasound2 libpangocairo-1.0-0 libnss3 libpq-dev curl \
    && rm -rf /var/lib/apt/lists/*

# Installer Playwright
RUN pip install playwright==1.34.1
RUN playwright install --with-deps

# Installer Crawl4AI
RUN pip install crawl4ai==3.0.0

# Cr√©er le r√©pertoire d'app
WORKDIR /app

# Copier la configuration du scraper
COPY config.json /app/config.json
COPY run_crawl4ai.py /app/run_crawl4ai.py

# Cr√©er un volume pour la base de donn√©es (mont√© par docker-compose)
VOLUME ["/app/app.db"]

# Point d'entr√©e
CMD ["python", "run_crawl4ai.py", "--config", "config.json"]
```

> **Explications** :
>
> * On utilise l‚Äôimage `python:3.10-slim` pour un conteneur plus l√©ger.
> * On installe les d√©pendances syst√®me n√©cessaires √† Playwright.
> * On installe `crawl4ai` et ses d√©pendances.
> * Le script `run_crawl4ai.py` sera ex√©cut√© automatiquement au d√©marrage du conteneur.
> * Le volume `/app/app.db` sera mont√© depuis le dossier `api/` (shared).

---

### 5.3. Exemple de Script Python pour Crawl4AI (`run_crawl4ai.py`)

```python
import os
import json
import sqlite3
import logging
from crawl4ai import Crawler
from crawl4ai.config import CrawlerConfig
import time
import requests

# Configuration Logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Crawl4AI")

# Variables d'environnement
DB_PATH = os.getenv("DB_PATH", "/app/app.db")
BOT_ADMIN_URL = os.getenv("BOT_ADMIN_URL")  # ex: http://bot_admin:5001/alert

# Charger configuration JSON
with open("config.json", "r") as f:
    config_data = json.load(f)

def init_db():
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS offers (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE NOT NULL,
            title TEXT NOT NULL,
            company TEXT,
            location TEXT,
            date_posted TEXT,
            contract_type TEXT,
            date_scraped TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            is_new BOOLEAN DEFAULT 1
        );
    """)
    conn.commit()
    conn.close()

def insert_offers(records):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    for rec in records:
        try:
            cursor.execute("""
                INSERT OR IGNORE INTO offers (url, title, company, location, date_posted, contract_type, is_new)
                VALUES (?, ?, ?, ?, ?, ?, 1)
            """, (
                rec.get("url", ""),
                rec.get("title", ""),
                rec.get("company", ""),
                rec.get("location", ""),
                rec.get("date_posted", ""),
                rec.get("contract_type", "")
            ))
        except Exception as e:
            logger.error(f"Erreur insertion DB : {e} - {rec}")
    conn.commit()
    conn.close()

def alert_admin(site_name, error_msg):
    try:
        payload = {"site": site_name, "error": error_msg}
        requests.post(BOT_ADMIN_URL, json=payload, timeout=5)
        logger.info(f"Alerte envoy√©e : {site_name} -> {error_msg}")
    except Exception as e:
        logger.error(f"Impossible d'alerter admin : {e}")

def run_crawler_for_site(site_cfg):
    try:
        site_name = site_cfg["name"]
        url = site_cfg["url"]
        prompt = site_cfg["prompt"]

        # Initialisation config Crawl4AI
        cfg = CrawlerConfig(
            url=url,
            prompt=prompt,
            llm_provider=config_data.get("llm_provider"),
            llm_model=config_data.get("llm_model"),
            llm_api_key=os.getenv("LLM_API_KEY")
        )
        crawler = Crawler(cfg)

        # Ex√©cution du crawl
        logger.info(f"D√©but crawl site : {site_name} ({url})")
        records = crawler.arun()  # r√©cuperation liste dicts
        logger.info(f"{len(records)} offres r√©cup√©r√©es pour {site_name}")

        # Insertion en base
        insert_offers(records)
    except Exception as e:
        err_msg = str(e)
        logger.error(f"Erreur sur site {site_cfg['name']} : {err_msg}")
        alert_admin(site_cfg["name"], err_msg)

def main():
    init_db()
    sites = config_data.get("sites", [])
    for site_cfg in sites:
        run_crawler_for_site(site_cfg)
        # Pause entre chaque site pour limiter la charge
        time.sleep(10)

if __name__ == "__main__":
    main()
```

> **Explications** :
>
> * `CrawlerConfig` : objet de configuration pour Crawl4AI (URL, prompt, LLM).
> * M√©thode `crawler.arun()` : renvoie une liste de dictionnaires, chaque dict ayant les cl√©s `url`, `title`, `company`, `location`, `date_posted`, `contract_type`.
> * On appelle `init_db()` pour s‚Äôassurer que la table `offers` existe.
> * Pour chaque site, on ex√©cute un crawl, on r√©cup√®re les offres et on ins√®re en base (via `INSERT OR IGNORE` pour g√©rer le delta).
> * En cas d‚Äôerreur (CSS introuvable, code HTTP 404, etc.), on appelle `alert_admin()`, qui poste un JSON √† `BOT_ADMIN_URL`.

---

### 5.4. Exemple de Dockerfile pour l‚ÄôAPI FastAPI

```
# Fichier : api/Dockerfile
FROM tiangolo/uvicorn-gunicorn-fastapi:python3.10

# Cr√©er le r√©pertoire de l'application
WORKDIR /app

# Copier requirements et installer
COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

# Copier l'application
COPY main.py /app/main.py

# Cr√©er un volume pour la base de donn√©es
VOLUME ["/app/app.db"]

# Expose port (d√©fini dans l'image de base)
# CMD d√©j√† d√©fini dans l'image de base (uvicorn main:app)

```

* `requirements.txt` :

  ```
  fastapi
  uvicorn[standard]
  pydantic
  sqlite3  # pour SQLite (d√©j√† int√©gr√© dans Python)
  databases  # si PostgreSQL plus tard
  ```

---

### 5.5. Exemple de Dockerfile pour le Bot Telegram ‚ÄúOffres‚Äù

```
# Fichier : bot_offres/Dockerfile
FROM python:3.10-slim

ENV PIP_NO_CACHE_DIR=1 \
    PYTHONUNBUFFERED=1

WORKDIR /app

COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

COPY bot_offres.py /app/bot_offres.py

CMD ["python", "bot_offres.py"]
```

* `requirements.txt` :

  ```
  python-telegram-bot==20.3
  apscheduler
  requests
  ```
* `bot_offres.py` : voir section [4.4.1.2](#441-exemple-de-code-bot-telegrams-offres).

---

### 5.6. Exemple de Dockerfile pour le Bot WhatsApp (Twilio)

```
# Fichier : bot_whatsapp/Dockerfile
FROM python:3.10-slim

ENV PIP_NO_CACHE_DIR=1 \
    PYTHONUNBUFFERED=1

WORKDIR /app

COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

COPY bot_whatsapp.py /app/bot_whatsapp.py

CMD ["python", "bot_whatsapp.py"]
```

* `requirements.txt` :

  ```
  twilio
  apscheduler
  requests
  ```

---

### 5.7. Exemple de Dockerfile pour le Bot Admin

```
# Fichier : bot_admin/Dockerfile
FROM python:3.10-slim

ENV PIP_NO_CACHE_DIR=1 \
    PYTHONUNBUFFERED=1

WORKDIR /app

COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

COPY bot_admin.py /app/bot_admin.py

CMD ["python", "bot_admin.py"]
```

* `requirements.txt` :

  ```
  flask
  python-telegram-bot==20.3
  requests
  ```

---

## 6. Plan de D√©ploiement & Mise en ≈íuvre

### 6.1. Pr√©paration du VPS (VLE-2)

1. **Acc√®s SSH** :

   ```bash
   ssh root@<IP_VPS>
   ```
2. **Mettre √† jour le syst√®me** :

   ```bash
   apt update && apt upgrade -y
   ```
3. **Installer les packages de base** :

   ```bash
   apt install -y git curl wget build-essential apt-transport-https ca-certificates gnupg lsb-release
   ```

### 6.2. Installation de Docker & Docker Compose

1. **Installer Docker** :

   ```bash
   curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
   add-apt-repository \
     "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
     $(lsb_release -cs) \
     stable"
   apt update
   apt install -y docker-ce docker-ce-cli containerd.io
   ```
2. **Ajouter ton utilisateur au groupe Docker** (optionnel, si tu n‚Äôes pas root) :

   ```bash
   usermod -aG docker <TON_UTILISATEUR>
   ```
3. **Installer Docker Compose** :

   ```bash
   DOCKER_COMPOSE_VERSION="1.29.2"
   curl -L "https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
   chmod +x /usr/local/bin/docker-compose
   ```
4. **V√©rifier l‚Äôinstallation** :

   ```bash
   docker --version
   docker-compose --version
   ```

### 6.3. Structure des R√©pertoires sur le VPS

Sur ton VPS, choisis un dossier de travail, par exemple `/home/<user>/job-aggregator/`.

```bash
mkdir -p /home/<user>/job-aggregator
cd /home/<user>/job-aggregator
```

Clone (ou cr√©e) les r√©pertoires suivants :

```
job-aggregator/
‚îú‚îÄ crawler1/
‚îÇ  ‚îú‚îÄ Dockerfile
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îú‚îÄ config.json
‚îÇ  ‚îî‚îÄ run_crawl4ai.py
‚îú‚îÄ crawler2/
‚îÇ  ‚îî‚îÄ ... (m√™mes fichiers, config.json adapt√© aux sites 6-10)
‚îú‚îÄ crawler3/
‚îÇ  ‚îî‚îÄ ... (m√™mes fichiers, config.json adapt√© aux sites 11-15)
‚îú‚îÄ api/
‚îÇ  ‚îú‚îÄ Dockerfile
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îî‚îÄ main.py
‚îú‚îÄ bot_offres/
‚îÇ  ‚îú‚îÄ Dockerfile
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îî‚îÄ bot_offres.py
‚îú‚îÄ bot_whatsapp/
‚îÇ  ‚îú‚îÄ Dockerfile
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îî‚îÄ bot_whatsapp.py
‚îú‚îÄ bot_twitter/
‚îÇ  ‚îú‚îÄ Dockerfile
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îî‚îÄ bot_twitter.py
‚îú‚îÄ bot_admin/
‚îÇ  ‚îú‚îÄ Dockerfile
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îî‚îÄ bot_admin.py
‚îî‚îÄ docker-compose.yml
```

### 6.4. Configuration du Cron

1. **√âditer le crontab** :

   ```bash
   crontab -e
   ```
2. **Ajouter la ligne suivante** pour red√©marrer les conteneurs `crawler1`, `crawler2`, `crawler3` 4√ó/jour :

   ```
   0 0,6,12,18 * * * docker restart crawler1 crawler2 crawler3
   ```

   > Ce cron ex√©cute `docker restart` √† 00h, 06h, 12h, 18h pour red√©marrer les crawlers.
   > Au red√©marrage, chaque `run_crawl4ai.py` s‚Äôex√©cute automatiquement, scrappe ses sites et poste les donn√©es en DB.

### 6.5. D√©ploiement Frontend sur Vercel

1. **Cr√©er un compte Vercel** (gratuit).
2. **Cloner** ou **pousser** le r√©pertoire `frontend/` sur un repository GitHub (ou GitLab).
3. **Connecter** ce repo √† Vercel (import project).
4. **D√©finir** la variable d‚Äôenvironnement `NEXT_PUBLIC_API_URL = https://<IP_VPS>:8000` (ou utiliser un nom de domaine).
5. **D√©ployer** : Vercel construira automatiquement le projet Next.js et le servira.

   * R√©sultat : `https://<ton-projet>.vercel.app` montre la page d‚Äôaccueil.

> **Note** : si ton VPS n‚Äôa pas de nom de domaine, tu peux exposer l‚ÄôAPI via une IP publique (ex: `http://123.45.67.89:8000`). Pour la production, il est recommand√© de mettre un reverse-proxy (NGINX) et un nom de domaine, mais ce n‚Äôest pas indispensable pour le MVP.

---

## 7. Gestion du Stockage & Base de Donn√©es

### 7.1. Sch√©ma de la Base `offers`

```sql
CREATE TABLE IF NOT EXISTS offers (
  id              INTEGER PRIMARY KEY AUTOINCREMENT,
  url             TEXT UNIQUE NOT NULL,
  title           TEXT NOT NULL,
  company         TEXT,
  location        TEXT,
  date_posted     TEXT,
  contract_type   TEXT,
  date_scraped    TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  is_new          BOOLEAN DEFAULT 1
);
```

* **Champs cl√©s** :

  * `url` : identifiant unique (emp√™che doublons).
  * `date_posted` : date d‚Äôorigine si connue.
  * `date_scraped` : timestamp UTC d‚Äôinsertion.
  * `is_new` : bool√©en pour g√©rer la logique ‚Äúoffres non encore envoy√©es‚Äù aux bots.

### 7.2. Migration SQLite ‚Üí PostgreSQL (Phase 2)

1. Exporter les donn√©es SQLite :

   ```bash
   sqlite3 app.db .dump > sqlite_dump.sql
   ```
2. Sur PostgreSQL (ElephantSQL ou Heroku) :

   ```sql
   -- Exemple psql
   CREATE TABLE offers (
     id SERIAL PRIMARY KEY,
     url TEXT UNIQUE NOT NULL,
     title TEXT NOT NULL,
     company TEXT,
     location TEXT,
     date_posted TIMESTAMP,
     contract_type TEXT,
     date_scraped TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
     is_new BOOLEAN DEFAULT TRUE
   );

   \i sqlite_dump.sql
   ```
3. Adapter la connexion dans `FastAPI` :

   ```python
   import databases, sqlalchemy

   DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://user:pass@host:5432/dbname")
   database = databases.Database(DATABASE_URL)
   metadata = sqlalchemy.MetaData()
   # D√©finir la table offers via SQLAlchemy...
   ```
4. Mettre √† jour `docker-compose.yml` pour pointer vers un conteneur PostgreSQL (ou une URL manag√©e).
5. Tester les requ√™tes `/offers` sur PostgreSQL avant de retirer SQLite.

---

## 8. Surveillance, Logs & Alerte

### 8.1. Logs Docker & Heap

* **Logs des conteneurs** :

  ```bash
  docker logs crawler1 --follow
  docker logs api_service --follow
  docker logs bot_offres --follow
  ```
* **Surveillance m√©moire & CPU** :

  ```bash
  docker stats
  htop
  ```
* **Rotation des logs** :

  * Sur chaque conteneur, on peut configurer un **log-driver** (ex : `json-file` avec `max-size: "10m"` et `max-file: "3"`) dans `docker-compose.yml` :

    ```yaml
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    ```

### 8.2. Bot Admin : M√©canisme d‚ÄôAlerte

* **Endpoints** :

  * `http://bot_admin:5001/alert` re√ßoit un JSON `{ "site": "NomDuSite", "error": "Description de l'erreur" }`.
* **Fonctionnement** :

  * **Crawl4AI** : dans le bloc `except` du crawl, appeler `alert_admin(site_name, error_msg)` qui POSTe au Bot Admin.
  * **Bot Admin** (Flask) re√ßoit, formate un message **Markdown** et envoie sur le chat admin Telegram (ID dans `ADMIN_CHAT_ID`).
  * Optionnel : on peut stocker ces alertes dans une table `alerts` dans la base pour historique.

#### Exemple du handler d‚Äôerreur dans `run_crawl4ai.py`

```python
except Exception as e:
    err_msg = str(e)
    logger.error(f"Erreur sur site {site_cfg['name']} : {err_msg}")
    try:
        requests.post(
            os.getenv("BOT_ADMIN_URL"),
            json={"site": site_cfg["name"], "error": err_msg},
            timeout=5
        )
    except Exception as ex:
        logger.error(f"Impossible d‚Äôenvoyer l‚Äôalerte : {ex}")
```

---

### 8.3. Healthchecks & Red√©marrage Automatique

* **Configurer `restart: on-failure`** dans `docker-compose.yml` pour les crawlers et bots critiques :

  ```yaml
  crawler1:
    restart: on-failure
    ...
  api:
    restart: always
    ...
  ```
* **Healthcheck** (Docker Compose v3.9+) :

  ```yaml
  api:
    build: ./api
    container_name: api_service
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 1m
      timeout: 10s
      retries: 3
  ```

  * L‚ÄôAPI FastAPI doit exposer un endpoint `/health` renvoyant `{"status": "ok"}`.
* Si un conteneur √©choue 3 fois de suite, Docker ne tente plus de le red√©marrer, d√©clenchant une alerte du Bot Admin.

---

## 9. Estimation des Ressources & Co√ªts

### 9.1. Usage M√©moire et CPU

| Composant                   | RAM estim√©e | CPU estim√© (peak)     | Notes                                                               |
| --------------------------- | ----------- | --------------------- | ------------------------------------------------------------------- |
| **Crawl4AI (1 conteneur)**  | \~ 500 Mo   | \~ 10 % ‚Äì 15 %        | Crawl de 5 sites (mode headless + LLM).                             |
| **Crawl4AI (3 conteneurs)** | \~ 1,5 Go   | \~ 30 % ‚Äì 45 % (peak) | Peut √™tre ex√©cut√© en **s√©quentiel** pour une empreinte plus faible. |
| **API FastAPI**             | \~ 200 Mo   | \~ 5 % ‚Äì 10 %         | Idle / requ√™tes l√©g√®res.                                            |
| **Bot Telegram (offres)**   | \~ 100 Mo   | \~ 5 % ‚Äì 10 %         | Idle, ex√©cution d‚Äôappels HTTP toutes les heures.                    |
| **Bot WhatsApp (Twilio)**   | \~ 100 Mo   | \~ 5 % ‚Äì 10 %         | Idle, ex√©cution d‚Äôappels HTTP toutes les heures.                    |
| **Bot Twitter (X)**         | \~ 100 Mo   | \~ 5 % ‚Äì 10 %         | Idle, ex√©cution d‚Äôappels HTTP toutes les heures.                    |
| **Bot Admin (Flask)**       | \~ 50 Mo    | \~ 2 % ‚Äì 5 %          | Idle, n‚Äôenvoie qu‚Äôen cas d‚Äôerreur.                                  |
| **OS + Docker Engine**      | \~ 200 Mo   | \~ 5 % ‚Äì 10 %         | Ubuntu minimal + Docker.                                            |
| **Total approxim√©**         | \~ 2,05 Go  | \~ 70 % ‚Äì 90 % (peak) | Reste < 200 Mo pour swap et pics.                                   |

> **Remarques** :
>
> * Pour limiter l‚Äôusage m√©moire, on peut lancer **deux crawlers en cascade** plut√¥t que 3 en parall√®le.
> * On peut regrouper certains bots (ex. Telegram ‚Äúoffres‚Äù + Admin) dans un m√™me conteneur pour √©conomiser 50 Mo.
> * Surveiller r√©guli√®rement avec `docker stats`, `htop`, et ajuster si n√©cessaire.

---

### 9.2. Bande Passante & Stockage

* **Stockage** :

  * 40 Go SSD NVMe sur le VPS : suffisant pour **SQLite (quelques milliers d‚Äôenregistrements)**, logs Docker, scripts.
  * Compression automatique possible (`VACUUM` SQLite).
* **Bande Passante** :

  * 500 Mbit/s illimit√© ‚Äì pas de co√ªt additionnel pour le crawling (quelques centaines de Mo de data / jour max).
  * Les bots consomment tr√®s peu (quelques Ko / message).

---

### 9.3. Budget Mensuel

| √âl√©ment                            | Co√ªt mensuel estim√©                                                |
| ---------------------------------- | ------------------------------------------------------------------ |
| **VPS (VLE-2)**                    | 4,25 ‚Ç¨ HT (‚âà 5,10 ‚Ç¨ TTC)                                           |
| **Nom de domaine (optionnel)**     | \~ 1 ‚Ç¨ ‚Äì 2 ‚Ç¨                                                       |
| **Twilio WhatsApp** (usage limit√©) | Gratuit (jusqu‚Äô√† un certain quota)                                 |
| **API OpenRouter (LLM)**           | Gratuit (quelques milliers d‚Äôappels) ‚Äì surveiller le quota mensuel |
| **Total (hors domaine)**           | **\~ 5 ‚Ç¨ ‚Äì 7 ‚Ç¨**                                                   |

> **√Ä noter** :
>
> * Si tu d√©passes le quota OpenRouter, un abonnement modeste (10 ‚Äì 20 ‚Ç¨ / mois) couvre un usage plus intensif.
> * Pour un MVP, la configuration actuelle reste tr√®s bon march√© (< 7 ‚Ç¨ / mois).

---

## 10. Roadmap & Phases d‚Äô√âvolution

### 10.1. Phase 1 : MVP Essentiel (Semaines 1 ‚Äì 4)

| T√¢che                                | Description                                                                                         | Dur√©e estim√©e |
| ------------------------------------ | --------------------------------------------------------------------------------------------------- | ------------- |
| **A. Provision VPS & Setup Initial** | Installation Docker, Docker Compose, structure des r√©pertoires, .env, cl√©s API.                     | 1 jour        |
| **B. Scraper IA-first (Crawl4AI)**   | 3 conteneurs (5 sites chacun), configuration `config.json`, `run_crawl4ai.py`, tests unitaires.     | 4 jours       |
| **C. API FastAPI**                   | Endpoints : `/offers`, `/offers?nouveaux=true`, `/offer/{id}`, `/filters`. Tests manuels.           | 3 jours       |
| **D. Mini-Site Next.js**             | Pages : accueil (`/`, filtres, pagination), d√©tail (`/offer/[id]`), responsive. D√©ploiement Vercel. | 5 jours       |
| **E. Bot Telegram (Offres)**         | Script Python, scheduler, test en local, d√©ploiement Docker.                                        | 2 jours       |
| **F. Bot Admin (Alertes)**           | Micro-service Flask, gestion erreurs, int√©gration avec Crawl4AI, test.                              | 2 jours       |
| **G. Bot WhatsApp (optionnel MVP)**  | Script Python Twilio, test, d√©ploiement.                                                            | 2 jours       |
| **H. Docker Compose & Cron**         | R√©daction du `docker-compose.yml`, config `crontab`, tests bout en bout.                            | 2 jours       |
| **I. Tests & Validation**            | Tests end-to-end : scraping ‚Üí DB ‚Üí API ‚Üí frontend ‚Üí bots.                                           | 2 jours       |

> **Total Phase 1 : 23 jours environ** (d√©veloppeur solo, avec prompts IA pour acc√©l√©rer la g√©n√©ration de code).

---

### 10.2. Phase 2 : Am√©liorations IA & Performance (Semaines 5 ‚Äì 8)

| T√¢che                                            | Description                                                                                                     | Dur√©e estim√©e |
| ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- | ------------- |
| **A. Optimisation du Scraping**                  | R√©duire √† 2 conteneurs si n√©cessaire, ajuster `time.sleep`, optimiser prompts LLM.                              | 3 jours       |
| **B. Migration SQLite ‚Üí PostgreSQL**             | Export/import, mise √† jour des scripts FastAPI, tests de requ√™tes.                                              | 4 jours       |
| **C. Ajout de Fuse.js pour recherche full-text** | Int√©grer Fuse.js dans Next.js pour recherche instantan√©e sans appeler l‚ÄôAPI.                                    | 3 jours       |
| **D. Filtres avanc√©s & UI**                      | Ajouter multiselect (location, contrat), autocompl√©tion dans le champ recherche, loader (skeleton).             | 4 jours       |
| **E. Monitoring & Healthchecks**                 | Configurer `healthcheck` pour API, ajouter alertes au Bot Admin si conteneur down plus de 5 min, logs rotatifs. | 3 jours       |
| **F. Tests automatis√©s**                         | √âcrire tests `pytest` pour API, tests Playwright pour UI, tests unitaires pour scrapers.                        | 4 jours       |
| **G. Automatisation CI/CD**                      | GitHub Actions pour build Docker, push sur Docker Hub, d√©ploiement auto sur VPS (via Docker Compose).           | 3 jours       |
| **H. Documentation & Guides**                    | R√©diger README global, inclure prompts IA, ajouter guide de d√©ploiement, sch√©mas Mermaid mis √† jour.            | 2 jours       |

> **Total Phase 2 : \~ 22 jours**

---

### 10.3. Phase 3 : Extensions & Modules Premium (Semaines 9 ‚Äì 16)

| T√¢che                                               | Description                                                                                                                   | Dur√©e estim√©e |
| --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ------------- |
| **A. Microservices**                                | D√©couper monolithe en services : ScraperService, ApiService, BotService, AdminService. D√©ployer sur Kubernetes (optionnel).   | 6 jours       |
| **B. Cat√©gorisation IA (spaCy / HuggingFace)**      | Int√©grer un mod√®le local (spaCy) pour extraire ‚Äúsecteur‚Äù, ‚Äúniveaux d‚Äôexp√©rience‚Äù.                                             | 4 jours       |
| **C. Recherche Hybride (ElasticSearch / Weaviate)** | Installer ElasticSearch / Weaviate, indexer les offres, adapter API pour requ√™tes full-text + vectorielles.                   | 5 jours       |
| **D. Plugin ‚ÄúRecruteur‚Äù Minimal**                   | Back-office Next.js : authentification basique (token), formulaire de publication, section statistiques.                      | 6 jours       |
| **E. Coaching IA (Candidate Coach)**                | Module IA g√©n√©rant lettre de motivation, analyse de CV (prompt IA) sur un endpoint `/ai`.                                     | 5 jours       |
| **F. Marketplace & Paiement**                       | Int√©grer Stripe/PayPal, panel de gestion pour recruteurs, passerelle de paiement, facturation mensuelle.                      | 4 jours       |
| **G. Monitoring Avanc√© (Prometheus + Grafana)**     | Installer Prometheus / Grafana, config dashboards (CPU, RAM, latence), alertes Slack/Telegram.                                | 4 jours       |
| **H. Tests de Charge**                              | Utiliser Locust ou k6 pour tests 100 req/s sur API `/offers`, crawler sous charge, optimiser indexes.                         | 3 jours       |
| **I. S√©curit√© & RGPD**                              | Audits l√©gers (OWASP Top 10), impl√©menter HTTPS (via NGINX), g√©rer TOS/Privacy pour espace recruteur, cliquer infos perso.    | 4 jours       |
| **J. Documentation Compl√®te**                       | G√©n√©rer la documentation API (OpenAPI), ajouter sections ‚ÄúD√©veloppement avanc√©‚Äù, ‚ÄúArchitecture microservices‚Äù, ‚ÄúSDK clients‚Äù. | 3 jours       |

> **Total Phase 3 : \~ 44 jours**

---

## 11. Annexes & Ressources Utiles

### 11.1. Prompts IA Recommand√©s

#### 11.1.1. Prompt pour g√©n√©rer un `Dockerfile` Crawl4AI

```
√âcris un Dockerfile pour Crawl4AI v3.0 qui :
- Part de l'image `python:3.10-slim`.
- Installe les d√©pendances syst√®me n√©cessaires pour Playwright (chromium, etc.).
- Installe Playwright et Crawl4AI via pip.
- Copie `config.json` et `run_crawl4ai.py` dans `/app`.
- D√©finit `/app/app.db` comme volume pour la base.
- Met la commande par d√©faut : `python /app/run_crawl4ai.py --config /app/config.json`.
```

#### 11.1.2. Prompt pour g√©n√©rer l‚ÄôAPI FastAPI

```
Cr√©e un projet FastAPI qui :
- Se connecte √† un fichier SQLite `app.db` contenant la table `offers`.
- Expose 3 endpoints :
  1. GET /offers?page={int}&page_size={int}&location={opt}&contract_type={opt} 
     ‚Üí renvoie les offres tri√©es par date_scraped DESC.
  2. GET /offers?nouveaux=true 
     ‚Üí renvoie toutes les offres avec is_new = 1 et met √† jour is_new = 0.
  3. GET /offer/{id} 
     ‚Üí renvoie les d√©tails d'une offre donn√©e par son id.
  4. GET /filters 
     ‚Üí renvoie une liste JSON distincte des `location` et `contract_type`.
- Documente automatiquement via Swagger.
R√©ponds uniquement avec le code Python complet (fichier main.py et requirements.txt).
```

#### 11.1.3. Prompt pour g√©n√©rer le Bot Telegram

```
Cr√©e un script Python (`bot_offres.py`) utilisant `python-telegram-bot` qui :
- Lit `TELEGRAM_TOKEN` de l'environnement, ainsi que `API_URL`.
- Sur la commande `/start`, envoie un message d'accueil expliquant le fonctionnement.
- Toutes les heures, appelle `GET {API_URL}/offers?nouveaux=true` et pour chaque offre retourn√©e, envoie :
  "üìå <title> ‚Äì <company> (<location>)\n‚è∞ Publi√©e : <date_posted>\nüîó <url>" 
  en Markdown au chat ID stock√© dans `TG_CHANNEL_ID`.
R√©ponds uniquement avec le code complet.
```

#### 11.1.4. Prompt pour g√©n√©rer le Bot WhatsApp

```
Cr√©√© un script Python (`bot_whatsapp.py`) utilisant la lib Twilio qui :
- Lit `TWILIO_ACCOUNT_SID`, `TWILIO_AUTH_TOKEN`, `WHATSAPP_FROM`, `WHATSAPP_TO`, `API_URL` depuis l'environnement.
- Toutes les heures, appelle `GET {API_URL}/offers?nouveaux=true` et pour chaque offre, envoie un message WhatsApp :
  "<title> ‚Äì <company> (<location>)\nPubli√© : <date_posted>\nVoir : <url>".
R√©ponds uniquement avec le code complet.
```

---

### 11.2. Biblioth√®ques & D√©p√¥ts GitHub Conseill√©s

* **Crawl4AI** : [https://github.com/Crawl4AI/crawl4ai](https://github.com/Crawl4AI/crawl4ai)
* **ScrapeGraphAI** : [https://github.com/MarketSquare/scrapegraphai](https://github.com/MarketSquare/scrapegraphai)
* **FastAPI** : [https://github.com/tiangolo/fastapi](https://github.com/tiangolo/fastapi)
* **python-telegram-bot** : [https://github.com/python-telegram-bot/python-telegram-bot](https://github.com/python-telegram-bot/python-telegram-bot)
* **Twilio Python** : [https://github.com/twilio/twilio-python](https://github.com/twilio/twilio-python)
* **Next.js** : [https://github.com/vercel/next.js](https://github.com/vercel/next.js)
* **Tailwind CSS** : [https://github.com/tailwindlabs/tailwindcss](https://github.com/tailwindlabs/tailwindcss)
* **APScheduler** : [https://github.com/agronholm/apscheduler](https://github.com/agronholm/apscheduler)
* **Tweepy (Twitter)** : [https://github.com/tweepy/tweepy](https://github.com/tweepy/tweepy)

---

### 11.3. Checklist de Validation

Avant de d√©clarer la mise en production du MVP, v√©rifie :

1. üü¢ **Scraping**

   * [ ] Les 3 conteneurs Crawl4AI s‚Äôex√©cutent sans erreur sur 5 sites chacun.
   * [ ] Seules les nouvelles offres apparaissent dans la base (`is_new` devient 0 apr√®s lecture).
   * [ ] Les erreurs de scraping (CSS manquant, page 404) g√©n√®rent bien une alerte via Bot Admin.

2. üü¢ **Base de Donn√©es**

   * [ ] Table `offers` cr√©√©e et populeÃÅe correctement.
   * [ ] Les colonnes `title`, `company`, `location`, `date_posted`, `contract_type`, `url` sont remplies.

3. üü¢ **API**

   * [ ] `GET /offers` fonctionne avec pagination et filtres `location` / `contract_type`.
   * [ ] `GET /offers?nouveaux=true` retourne les offres `is_new = 1` et les marque `is_new = 0`.
   * [ ] `GET /offer/{id}` retourne les d√©tails complets.
   * [ ] `GET /filters` retourne la liste `locations` et `contract_types` actuelles.

4. üü¢ **Mini-Site Next.js**

   * [ ] La page d‚Äôaccueil (`/`) affiche une liste pagin√©e d‚Äôoffres.
   * [ ] Les filtres fonctionnent (s√©lection de `location` / `contract_type`).
   * [ ] La recherche par mot-cl√© fonctionne (Fuse.js ou backend).
   * [ ] La page d√©tail (`/offer/[id]`) affiche correctement les champs et un bouton ‚ÄúVoir d√©tails‚Äù.

5. üü¢ **Bots de Diffusion**

   * [ ] Bot Telegram envoie bien les nouvelles offres toutes les heures.
   * [ ] Bot WhatsApp (Twilio) envoie l‚Äôalerte pour chaque nouvelle offre.
   * [ ] Bot Twitter publie un tweet pour chaque nouvelle offre (optionnel).
   * [ ] Aucun doublon : chaque offre n‚Äôest envoy√©e qu‚Äôune seule fois.

6. üü¢ **Bot Admin**

   * [ ] En cas d‚Äôerreur de scraping, un message est envoy√© au canal admin.
   * [ ] Si un crawler red√©marre 3 fois en moins de 30 minutes, Bot Admin informe.

7. üü¢ **D√©ploiement & Monitoring**

   * [ ] Tous les conteneurs se lancent sans erreur (`docker-compose up --build`).
   * [ ] Les logs Docker sont visibles et rotatifs (max-size=10m, max-file=3).
   * [ ] Le cron red√©marre bien les crawlers aux horaires pr√©vus.
   * [ ] Surveillance server (htop, docker stats) sans OOM ou pics trop fr√©quents.

---

## Conclusion

Cette documentation regroupe **l‚Äôensemble des parties** : du choix du scraper IA-first (Crawl4AI), √† l‚Äôarchitecture modulaire (Scraping ‚Üí DB ‚Üí API ‚Üí Frontend ‚Üí Bots), en passant par la configuration d√©taill√©e (Docker, scripts, cron), l‚Äôestimation des ressources, le plan de d√©ploiement, et la roadmap d‚Äô√©volution.

En respectant chacune des √©tapes, tu disposeras d‚Äôun MVP fonctionnel en **environ 23 jours** de travail solo **(Phase 1)**, pour un co√ªt mensuel tr√®s bas (**\~ 5 ‚Ç¨ ‚Äì 7 ‚Ç¨**) et une maintenance r√©duite.

Les **phases suivantes** (2 et 3) te permettront d‚Äôajouter progressivement de l‚ÄôIA avanc√©e (recherche s√©mantique, back-office recruteur, coaching IA) tout en gardant une base solide et un d√©ploiement automatis√©.

Bonne mise en ≈ìuvre et n‚Äôh√©site pas √† revenir pour toute pr√©cision compl√©mentaire !
