#!/usr/bin/env python3
"""
CRAWLER UNIFI√â - Fusion des deux architectures + Am√©liorations Phase 2

Ce crawler combine :
‚úÖ Le moteur Crawl4AI qui fonctionne (main_crawler.py)
‚úÖ Le syst√®me multi-sources (AbstractSource)
‚úÖ Les am√©liorations Phase 2 (classification + g√©olocalisation + nettoyage HTML)

Usage: python crawler/main_crawler_unified.py
"""

import asyncio
import json
import os
import importlib
import pkgutil
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawler.models import JobOffer
from crawler.llm_enrichment import GeminiEnricher
from crawler.supabase_export import upsert_job_to_supabase
from crawler.utils.job_classifier import JobClassifier
from crawler.utils.geo_extractor import GeoExtractor
from crawler.utils.intelligent_extractor import IntelligentExtractor
from crawler.utils.html_cleaner import clean_html_content
from bs4 import BeautifulSoup
import csv

def discover_sources():
    """D√©couvre automatiquement toutes les sources disponibles"""
    sources = []
    sources_dir = "crawler.sources"
    
    try:
        import crawler.sources
        for _, module_name, _ in pkgutil.iter_modules(crawler.sources.__path__):
            try:
                module = importlib.import_module(f"{sources_dir}.{module_name}")
                for attr_name in dir(module):
                    attr = getattr(module, attr_name)
                    if (isinstance(attr, type) and 
                        hasattr(attr, '__bases__') and 
                        any('AbstractSource' in str(base) for base in attr.__bases__)):
                        try:
                            source_instance = attr()
                            sources.append(source_instance)
                            print(f"‚úÖ Source d√©couverte : {source_instance.name}")
                        except Exception as e:
                            print(f"‚ö†Ô∏è  Erreur lors de l'initialisation de {attr.__name__}: {e}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors de l'import du module {module_name}: {e}")
    except Exception as e:
        print(f"‚ùå Erreur lors de la d√©couverte des sources: {e}")
    
    return sources

async def process_source(source, crawler, enricher, classifier, geo_extractor, intelligent_extractor, audit_records):
    """Traite une source sp√©cifique avec toutes les am√©liorations Phase 2"""
    
    print(f"\nüîç === TRAITEMENT SOURCE: {source.name.upper()} ===")
    
    LAST_SCRAP_FILE = "last_scrap.json"
    
    def get_last_scrap():
        if not os.path.exists(LAST_SCRAP_FILE):
            return None
        with open(LAST_SCRAP_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data.get(source.name)

    def set_last_scrap(new_date):
        data = {}
        if os.path.exists(LAST_SCRAP_FILE):
            with open(LAST_SCRAP_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
        data[source.name] = new_date
        with open(LAST_SCRAP_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f)

    last_scrap = get_last_scrap()
    print(f"[DELTA SCRAP] Dernier scrap pour {source.name} : {last_scrap}")
    
    total_processed = 0
    
    for listing_url in source.get_listing_urls():
        print(f"\nüìã Crawling listing: {listing_url}")
        
        listing_schema = source.get_listing_schema()
        extraction = JsonCssExtractionStrategy(listing_schema)
        crawl_cfg = CrawlerRunConfig(
            extraction_strategy=extraction,
            cache_mode=CacheMode.BYPASS,
            remove_overlay_elements=True,
            exclude_external_links=True,
            verbose=True,
            wait_for=f"css={listing_schema['baseSelector']}"
        )
        
        result = await crawler.arun(listing_url, config=crawl_cfg)
        
        if not result.success or not result.extracted_content:
            print(f"‚ùå √âchec crawl listing {listing_url}")
            continue
            
        try:
            data = json.loads(result.extracted_content)
            print(f"üìä {len(data)} offres extraites du listing")
            
            for i, item in enumerate(data):
                job_url = item.get("url")
                if not job_url:
                    print(f"Offre {i+1} sans URL, ignor√©e.")
                    continue
                
                print(f"\n--- Offre {i+1}/{len(data)} ---")
                print(f"üîó URL: {job_url}")
                
                # === CRAWL DES D√âTAILS DE L'OFFRE ===
                print("üìÑ R√©cup√©ration des d√©tails...")
                detail_result = None  # Initialiser la variable
                try:
                    # Construire l'URL compl√®te si n√©cessaire
                    if not job_url.startswith('http'):
                        if source.name == "emploi_tg":
                            detail_url = f"https://www.emploi.tg{job_url}"
                        elif source.name == "emploitogo_info":
                            detail_url = job_url  # D√©j√† une URL compl√®te
                        else:
                            detail_url = f"https://{source.name}.com{job_url}"
                    else:
                        detail_url = job_url
                    
                    # Configuration pour le crawl des d√©tails
                    detail_schema = source.get_detail_schema()
                    detail_extraction = JsonCssExtractionStrategy(detail_schema)
                    detail_crawl_cfg = CrawlerRunConfig(
                        extraction_strategy=detail_extraction,
                        cache_mode=CacheMode.BYPASS,
                        remove_overlay_elements=True,
                        exclude_external_links=True,
                        verbose=False,  # Moins verbeux pour les d√©tails
                        wait_for=f"css={detail_schema['baseSelector']}"
                    )
                    
                    detail_result = await crawler.arun(detail_url, config=detail_crawl_cfg)
                    
                    if detail_result.success and detail_result.extracted_content:
                        try:
                            detail_data = json.loads(detail_result.extracted_content)
                            if isinstance(detail_data, list) and len(detail_data) > 0:
                                detail_data = detail_data[0]  # Prendre le premier √©l√©ment
                            
                            # Fusionner les donn√©es de listing avec les d√©tails
                            item.update(detail_data)
                            print("   ‚îî‚îÄ ‚úÖ D√©tails r√©cup√©r√©s")
                        except json.JSONDecodeError:
                            print("   ‚îî‚îÄ ‚ö†Ô∏è  Erreur parsing JSON d√©tails")
                    else:
                        print("   ‚îî‚îÄ ‚ö†Ô∏è  √âchec r√©cup√©ration d√©tails")
                        
                except Exception as e:
                    print(f"   ‚îî‚îÄ ‚ùå Erreur crawl d√©tails: {e}")
                    detail_result = None  # S'assurer que detail_result est None en cas d'erreur
                
                # === AM√âLIORATIONS PHASE 2 ===
                
                # 0. EXTRACTION INTELLIGENTE DES DONN√âES MANQUANTES
                print("üß† Extraction intelligente des donn√©es...")
                if detail_result and detail_result.html:
                    # Utiliser l'extracteur intelligent pour compl√©ter les donn√©es manquantes
                    intelligent_data = intelligent_extractor.extract_all_data(detail_result.html, item)
                    
                    # Fusionner les donn√©es intelligentes avec les donn√©es existantes
                    for key, value in intelligent_data.items():
                        if value and (not item.get(key) or len(str(value)) > len(str(item.get(key, '')))):
                            item[key] = value
                    
                    print(f"   ‚îî‚îÄ ‚úÖ Donn√©es compl√©t√©es intelligemment")
                    
                    # Afficher les nouvelles donn√©es trouv√©es
                    new_data_found = []
                    if intelligent_data.get('company_name'):
                        new_data_found.append(f"Entreprise: {intelligent_data['company_name'][:30]}...")
                    if intelligent_data.get('salary'):
                        new_data_found.append(f"Salaire: {intelligent_data['salary'][:20]}...")
                    if intelligent_data.get('contract_type'):
                        new_data_found.append(f"Contrat: {intelligent_data['contract_type']}")
                    if intelligent_data.get('contact_email'):
                        new_data_found.append(f"Email: {intelligent_data['contact_email']}")
                    
                    if new_data_found:
                        for data in new_data_found[:3]:  # Afficher max 3 √©l√©ments
                            print(f"     üìã {data}")
                
                # 1. CLASSIFICATION AUTOMATIQUE
                print("üè∑Ô∏è  Classification automatique...")
                classification_result = classifier.classify_offer(
                    item.get('title', ''),
                    item.get('job_description', ''),
                    item.get('company_name', '')
                )
                item['offer_category'] = classification_result
                print(f"   ‚îî‚îÄ Cat√©gorie: {classification_result}")
                
                # 2. EXTRACTION G√âOGRAPHIQUE
                print("üåç Extraction g√©ographique...")
                location_text = f"{item.get('location', '')} {item.get('job_description', '')}"
                geo_result = geo_extractor.extract_location_info(location_text)
                
                # Ajouter les donn√©es g√©ographiques
                item['detected_city'] = geo_result['city']
                item['detected_region'] = geo_result['region']
                item['detected_latitude'] = geo_result['latitude']
                item['detected_longitude'] = geo_result['longitude']
                item['remote_work_detected'] = geo_result['is_remote']
                
                if geo_result['city']:
                    print(f"   ‚îî‚îÄ Ville: {geo_result['city']}, R√©gion: {geo_result['region']}")
                if geo_result['is_remote']:
                    print(f"   ‚îî‚îÄ T√©l√©travail d√©tect√©: {geo_result['is_remote']}")
                
                # 3. NETTOYAGE HTML
                print("üßπ Nettoyage HTML...")
                if item.get('job_description'):
                    item['job_description'] = clean_html_content(item['job_description'])
                
                # === ENRICHISSEMENT LLM (SI DISPONIBLE) ===
                if enricher:
                    print("ü§ñ Enrichissement LLM...")
                    try:
                        enriched_data = await enricher.enrich_job_offer(item)
                        item.update(enriched_data)
                        print("   ‚îî‚îÄ ‚úÖ Enrichissement LLM appliqu√©")
                    except Exception as e:
                        print(f"   ‚îî‚îÄ ‚ö†Ô∏è  Enrichissement LLM √©chou√©: {e}")
                
                # === EXPORT VERS SUPABASE ===
                print("üíæ Export vers Supabase...")
                try:
                    # Normaliser la date si n√©cessaire
                    normalized_date = item.get('date_posted')
                    if normalized_date and hasattr(source, 'normalize_date'):
                        normalized_date = source.normalize_date(normalized_date)
                    
                    # Cr√©er un objet JobOffer avec les donn√©es enrichies
                    job_offer = JobOffer(
                        title=item.get('title', ''),
                        company_name=item.get('company_name', ''),
                        location=item.get('location', ''),
                        job_description=item.get('job_description', ''),
                        source_url=detail_url,
                        date_posted=normalized_date,
                        # Nouveaux champs Phase 2
                        offer_category=item.get('offer_category'),
                        detected_city=item.get('detected_city'),
                        detected_region=item.get('detected_region'),
                        detected_latitude=item.get('detected_latitude'),
                        detected_longitude=item.get('detected_longitude'),
                        remote_work_detected=item.get('remote_work_detected', False),
                        # Champs enrichis LLM
                        company_logo_url=item.get('company_logo_url'),
                        company_website=item.get('company_website'),
                        company_description=item.get('company_description'),
                        skills=item.get('skills'),
                        salary=item.get('salary'),
                        languages=item.get('languages'),
                        sector=item.get('sector'),
                        education_level=item.get('education_level'),
                        experience_level=item.get('experience_level'),
                        contract_type=item.get('contract_type'),
                        remote_possible=item.get('remote_possible')
                    )
                    
                    # Convertir l'objet JobOffer en dictionnaire pour Supabase
                    job_dict = job_offer.model_dump()
                    
                    # Export vers Supabase
                    success = upsert_job_to_supabase(job_dict)
                    if success:
                        print("   ‚îî‚îÄ ‚úÖ Sauvegard√© en base de donn√©es")
                    else:
                        print("   ‚îî‚îÄ ‚ùå √âchec sauvegarde base de donn√©es")
                    
                except Exception as e:
                    print(f"   ‚îî‚îÄ ‚ùå Erreur export Supabase: {e}")
                
                print("‚úÖ Offre trait√©e avec am√©liorations Phase 2")
                total_processed += 1
                    
        except Exception as e:
            print(f"‚ùå Erreur d√©codage JSON listing: {e}")
            continue
    
    set_last_scrap(datetime.now().isoformat())
    print(f"\n‚úÖ Source {source.name} termin√©e: {total_processed} offres trait√©es")
    
    return total_processed

async def main():
    """Point d'entr√©e principal du crawler unifi√©"""
    
    print("üöÄ CRAWLER UNIFI√â - D√âMARRAGE")
    print("=" * 60)
    print("‚úÖ Moteur: Crawl4AI")
    print("‚úÖ Multi-sources: Activ√©")
    print("‚úÖ Phase 2: Classification + G√©olocalisation + Nettoyage HTML")
    print("‚úÖ Crawl d√©tails: ACTIV√â")
    print("‚úÖ Export: Supabase activ√©")
    print("=" * 60)
    
    browser_cfg = BrowserConfig(headless=True, verbose=True, text_mode=True)
    
    classifier = JobClassifier()
    geo_extractor = GeoExtractor()
    intelligent_extractor = IntelligentExtractor()
    print("‚úÖ Classification et g√©olocalisation initialis√©es")
    
    ENRICH_LLM = True
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    enricher = None
    
    if ENRICH_LLM and GEMINI_API_KEY:
        enricher = GeminiEnricher(api_key=GEMINI_API_KEY)
        print("‚úÖ Enrichissement LLM activ√©")
    else:
        print("‚ö†Ô∏è  Enrichissement LLM d√©sactiv√© (pas de cl√© API)")
    
    sources = discover_sources()
    if not sources:
        print("‚ùå Aucune source trouv√©e!")
        return
    
    print(f"üìã {len(sources)} source(s) d√©couverte(s)")
    
    audit_records = []
    total_global = 0
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        print("\nüî• D√âBUT DU CRAWLING MULTI-SOURCES")
        
        for source in sources:
            try:
                processed = await process_source(
                    source, crawler, enricher, classifier, geo_extractor, intelligent_extractor, audit_records
                )
                total_global += processed
            except Exception as e:
                print(f"‚ùå Erreur source {source.name}: {e}")
                continue
    
    print(f"\nüéâ CRAWLING TERMIN√â")
    print("=" * 60)
    print(f"üìä Total offres trait√©es: {total_global}")
    print(f"üìã Sources trait√©es: {len(sources)}")
    
    print("üéØ CRAWLER UNIFI√â TERMIN√â AVEC SUCC√àS!")

if __name__ == "__main__":
    asyncio.run(main()) 